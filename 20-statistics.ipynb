{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "962db9c3-eafd-47e9-bf3c-c9c82c091d20",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+3\"><strong>Statistics</strong></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db5063-136b-4b3d-a8e0-1c13632079ec",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02301f74-ea3e-42c9-b6b9-69c19f9f9aa6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cf29fd-9c65-44bf-90ec-fbf84a03ef6c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Normal Distribution\n",
    "\n",
    "Normal distribution is the most widely used continuous distribution. It's a bell-shaped distribution that has the following unique characteristics:\n",
    "\n",
    "1. Normal distributions are defined by only two parameters, the mean ($\\mu$) and the standard deviation ($\\sigma$).\n",
    "1. The mean, median and mode are all equal.\n",
    "1. The distribution is symmetric at the mean.\n",
    "1. 68% of the area of a normal distribution is within one standard deviation of the mean, 95% of the area is within two standard deviation of the mean, and 99.7% is within three standard deviations of the mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4af5a1-ccd2-4a4d-9a35-68f45f991d79",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The following graph shows an example of a Normal Distribution. You will see how it's bell shaped and symmetric at the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec6aed5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "![normal distribution](../images/normal-dist-pdf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8e16be-9291-408b-983f-a42b41e02a16",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# Define a normal distribution with mean 0 and standard deviation 1\n",
    "x = np.arange(-4, 4, 0.001)\n",
    "ax.plot(x, norm.pdf(x, loc=0, scale=1))\n",
    "ax.set_ylim(0, 0.45)\n",
    "\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"pdf(x)\")\n",
    "ax.grid(True)\n",
    "\n",
    "# fill one standard deviation of mean with red\n",
    "px = np.arange(-1, 1, 0.01)\n",
    "ax.fill_between(px, norm.pdf(px, loc=0, scale=1), alpha=0.5, color=\"r\")\n",
    "\n",
    "# fill two standard deviation of mean with blue\n",
    "px = np.arange(-2, -1, 0.01)\n",
    "ax.fill_between(px, norm.pdf(px, loc=0, scale=1), alpha=0.5, color=\"b\")\n",
    "\n",
    "px = np.arange(1, 2, 0.01)\n",
    "ax.fill_between(px, norm.pdf(px, loc=0, scale=1), alpha=0.5, color=\"b\")\n",
    "\n",
    "# fill two standard deviation of mean with green\n",
    "px = np.arange(-4, -2, 0.01)\n",
    "ax.fill_between(px, norm.pdf(px, loc=0, scale=1), alpha=0.5, color=\"g\")\n",
    "\n",
    "px = np.arange(2, 4, 0.01)\n",
    "ax.fill_between(px, norm.pdf(px, loc=0, scale=1), alpha=0.5, color=\"g\")\n",
    "\n",
    "# 68% of the area of a normal distribution is within one standard deviation of the mean\n",
    "ax.text(-0.5, 0.2, \"68%\", fontsize=20)\n",
    "# 95% of the area is within two standard deviation of the mean\n",
    "ax.text(-2.5, 0.01, \"5%\", fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb0b002-a502-4b5e-9cdd-5b6b2870ba9c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Probability Densities\n",
    "\n",
    "If you were an engineer designing the interior of an airplane, you'd need to balance precision and uncertainty. On the one hand, you'd need to provide exact specifications and measurements for the airplane seats, but those seats need to accommodate passengers whose height and weight you don't know. After all, there is a lot of variation from one passenger to the next! So how can you mathematically represent the measurements for height and weight in such a way that you can incorporate them into the engineering formulas needed to create your design specifications? The answer is a **probability density function (PDF)**.\n",
    "\n",
    "By definition, a probability density function shows the chance of observing an instance of random variable $X$ that equals a particular value $x$.\n",
    "\n",
    "$$ f(x) = P(X = x)$$\n",
    "\n",
    "Going back to our airplane seat example, $X$ could represent the height of an adult male passenger. It turns out that, if you measure thousands of American men, you'll end up with a mean height of 177cm and a standard deviation of 7cm. With that information, you can create a PDF that looks like a normal distribution.\n",
    "\n",
    "Using the SciPy library, we can use a probability density function to plot the distribution of heights for American men."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c0952e-1238-4a20-8841-d35777b25f77",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# define plot range for x axis\n",
    "x = np.linspace(149, 205, 1000)\n",
    "\n",
    "# plot PDF for a normal distribution with mean equals to 0 and standard deviation equals 1.\n",
    "y = norm.pdf(x, loc=177, scale=7)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Distribution of Heights for American Men\")\n",
    "plt.xlabel(\"Height [cm]\")\n",
    "plt.ylabel(\"Frequency [%]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a87963-35bb-4a5c-8523-107a06e31ab2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Cumulative Density Functions\n",
    "\n",
    "The Cumulative Density Function **CDF** is the function that evaluates the probability of the random variable $X$ takes on a value less than or equal to $x$ for a distribution. In formula, we have:\n",
    "\n",
    "$$ F_X(x) = P(X \\leq x)$$\n",
    "\n",
    "\n",
    "We can calculate the **CDF** of a normal distribution using `SciPy`. First we define a normal distribution from `Scipy` stats library. By default, the normal distribution has mean equals to 0 and standard deviation equals to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275ae299-7774-4841-9bd2-8b4eb1593901",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# get mean and standard deviation\n",
    "mean = norm.mean()\n",
    "std = norm.std()\n",
    "\n",
    "print(f\"The mean of this Normal Distribution is: {mean}\")\n",
    "print(f\"The standard deviation of this Normal Distribution is: {std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81905ca3-e008-4f4b-8128-7b3b389cc43b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "By definition, the area under the PDF is 1 by definition, and since the normal distribution is symmetric around the mean, the **CDF** at 0 should be 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9eb173-7163-4085-aab1-987639d76731",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Get `cdf` at 9\n",
    "cdf_value = norm.cdf(0)\n",
    "\n",
    "print(f\"CDF at 0 is: {cdf_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b5ba75-245d-4065-98db-e56da4ab5b3d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can also pass a list of values to check **CDF**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a471f40-aff8-4618-989a-2352235c114f",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "cdf_value = norm.cdf([-std, std])\n",
    "cdf_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dbef9f-67ab-4fc9-80aa-359bd4521be8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Note the difference between `norm.cdf(1)` and `norm.cdf(-1)` is the probability of $x$ being within one standard deviation of the mean, either left or right. According to normal distribution property, the probability should be around 68%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328672ab-3070-4c2e-a426-2d5cb4b1f78a",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "norm.cdf(1) - norm.cdf([-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb0a21d-615f-48d7-9802-9d7db533c735",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Calculate CDF for -2 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950ff04a-4d8c-4306-9689-e1b0cd555e5a",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "cdf_value = ...\n",
    "cdf_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d799bccf-1491-48d4-aea8-ce09ce164939",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Central Limit Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a43bd6-6dfc-42ec-a498-9726da8cccb9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The **Central Limit Theorem (CTL)** states that no matter what the population’s original distribution is, when taking random samples from the population, the distribution of the means or sums from the random samples approaches a normal distribution, where the mean equals the population mean, as the random sample size gets larger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd9b8bb-412b-461f-8605-13d63a198727",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## CTL for Random Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763606c5-6c50-4967-a601-ea6cf0496408",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can simulate the Central Limit Theorem by taking sub samples from a random sampled population, then taking the means of each sub sample and plot the means to see whether they follow a normal distribution. Here is the code for the simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c76de02-673a-4179-b65a-aa62952d076a",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# generate a sample from a random sample whose size is 10000\n",
    "lis = np.random.random(size=10000)\n",
    "plt.hist(lis);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17211608-6559-4683-9e30-db22e83bbd21",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can clearly see the distribution is not a Normal Distribution. Now let's take sub samples from the random sample, then take the mean of each sub sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eaa120-628e-4166-9e00-f2b71aee5cd6",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "means = []\n",
    "# take 1000 random sub sample, whose size is also 1000\n",
    "for i in range(1000):\n",
    "\n",
    "    # take a random sub sample by the index numbers of the previous random sample\n",
    "    lis_index = np.random.randint(0, 10000, 1000)\n",
    "\n",
    "    # collect the means from each sub sample\n",
    "    means.append(lis[lis_index].mean())\n",
    "\n",
    "    # continue taking sub samples and collect means\n",
    "    i += 1\n",
    "\n",
    "# plot the distribution of means\n",
    "plt.hist(means);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a412c3-3dfc-4899-b411-dda13c96cc5d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can see the distribution above has a bell shape, like the normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b02c3e2-a281-468c-952a-7b3b0876088f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## CTL for Sums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d1f67e-41fe-4455-937f-9b7a466695f7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Simply change mean to sum, we can see the central limit theorem also holds for sums:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b87d556-a5a0-42cc-bd6b-992aab1922a4",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "lis = np.random.random(size=10000)\n",
    "plt.hist(lis);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c2cbd-785d-4f5b-b1d7-c08caf603b45",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The above distribution is from a random sample, not a normal distribution for sure. Now we take sums from each sub samples like how we took means from the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be65e39-c2bd-47c6-8343-0712982b91db",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "sums = []\n",
    "# take 1000 random sub sample, whose size is also 1000\n",
    "for i in range(1000):\n",
    "    # take a random sub sample\n",
    "    lis_index = np.random.randint(0, 10000, 1000)\n",
    "\n",
    "    # collect the sums from each sub sample\n",
    "    sums.append(lis[lis_index].sum())\n",
    "    i += 1\n",
    "\n",
    "plt.hist(sums);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57985b9-d54e-438e-ae4c-3b5bfbd66886",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now we can see the distribution of sums is much like a bell shaped Normal Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ac2aa8-3062-41d1-9a5a-a64c24b8540c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Check out this YouTube video from Khan Academy for more detailed explanation in `Sampling`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82835f64-8ab5-4bb8-ad9d-38e497155fd1",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "YouTubeVideo(\"z0Ry_3_qhDw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c19cf66-d26b-4086-9fde-29d9aa2c2045",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Check out this YouTube video for an example of how to use the Central Limit Theorem for sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f5802e-5d9a-4a06-af5c-f82c724d2c95",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "YouTubeVideo(\"wY3TSCG-G3Q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c33d362-3e35-4755-9c70-0d599e3e8209",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64794749-28ce-409b-9fa9-041c4b196c3a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Hypothesis Testing** is a method of statistical inference. First, we define a **null hypothesis** ($H_0$), which usually proposes that no statistical relationship or significance exists. Next, we have an **alternative hypothesis** ($H_a$ or $H_a$), which proposes that there is a statistical relationship. For example, if we want to test whether there is a significant difference in salaries between employees with and without advanced degrees:\n",
    "\n",
    "- The **null hypothesis** is there **is no significant difference** in salaries between employees with and without advanced degrees;\n",
    "- The **alternative hypothesis** is there **is a significant difference** in salaries between employees with and without advanced degrees.\n",
    "\n",
    "Hypothesis testing is a two-step process that tests whether the null hypothesis is true based on data collected from a survey or an experiment. First, we calculate the probability of observing some effect in the data, assuming the null hypothesis is true. Second, we compare the probability from the previous calculation with the p-value we've chosen (usually 0.05). If the probability is smaller than the p-value, we reject the null hypothesis because the effect we're observing is probably true. If the probability is larger than the p-value, we fail to reject the null hypothesis because the effect we're observing is probably false. \n",
    "\n",
    "Either way, we can never be 100% certain our observed results are real, because we're dealing with probability. Even if we're almost certainly correct, the closest we can get is *almost*, because there's always a possibility — no matter how small — that our results are wrong. Maybe we found something that wasn't actually there, or we didn't find something that actually was. In statistics, we call those two mistakes **type errors**. They work like this:\n",
    "\n",
    "- In a **Type I error**, the null hypothesis is actually true, but we reject it, which is called **False Positive**, we also call it $\\alpha$;\n",
    "- In a **Type II error**, the null hypothesis is actually False, but we fail to reject it, which is called **False Negative**, we also call it $\\beta$;\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th style=\"text-align: left\"></th>\n",
    "<th style=\"text-align: left\">$H_0$ is rejected</th>\n",
    "<th style=\"text-align: left\">$H_0$ is not rejected</th>\n",
    "\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td style=\"text-align: left\">$H_0$ is False</td>\n",
    "<td style=\"text-align: left\">1- $\\beta$</td>\n",
    "<td style=\"text-align: left\">False Negative $\\beta$</td>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<td style=\"text-align: left\">$H_0$ is True</td>\n",
    "<td style=\"text-align: left\">False Positive $\\alpha$</td>\n",
    "<td style=\"text-align: left\">1 - $\\alpha$</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "    \n",
    "</table>\n",
    "\n",
    "For example, False Positive is when salaries between employees with and without advanced degrees are not significantly different ($H_0$ True), however, we conclude that they are significantly different (rejecting $H_0$). While False Negative is when salaries between employees with and without advanced degrees are significantly different ($H_0$ False), but we find that they are not significantly different (not rejecting $H_0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f981c-126e-4fe8-95e8-ec4cb4ac2404",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Power\n",
    "In the table above when we rejected $H_0$ when $H_0$ is False, the probability is 1-$\\beta$, which is also called the **statistical power** of the test. It represents the correct rejection of the null hypothesis. You want the value of the statistical power to be as large as possible for a test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817b88bf-03da-497c-b9a4-2166936259c8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Effect Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd24af58-a733-419c-a031-718e300c0969",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Effect size** measures the strength of a relationship between two variables or same variable from different samples. For example, we compare an experiment result from treatment group and control group, and we measure the different between the two groups to see the effect size of the experiment. One common measurement for effect size in this case is called **Cohen's d**, which measures the difference between two means from treatment and control group, divided by the standard deviation of the data:\n",
    " $$ d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8338a991-9110-418a-a899-6a8f4a6ada4f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Chi Square\n",
    "\n",
    "Among all hypothesis testing via statistical models, the **Chi-square test of independence** is widely used to test whether there is a significant relationship between two nominal variables. The Null Hypothesis $H_0$ and the Alternative Hypothesis $H_a$ for testing the independence between variable X and variable Y are framed as follows:\n",
    "\n",
    "- Null Hypothesis ($H_0$): There is no relationship between X and Y, i.e. X and Y are independent to each other;\n",
    "\n",
    "- Alternative Hypothesis ($H_a$): There is a relationship to X and Y.\n",
    "\n",
    "In the following section, we will use the Colombia real estate data set to illustrate the process of conducting a Chi-square test of independence. First, we read the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f7b222-2223-4a8f-970f-f55d67a1fb51",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/colombia-real-estate-chi-square.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11745650-9876-4fdf-8172-166a3d3a5362",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We will focus on two variables: `\"advertising\"` and `\"sold_in_2_wks\"`. There are two ways of advertising, through radio or internet. We want to test whether these two ways of advertising affects the probability of selling the property within two weeks. That is same as testing whether `\"advertising\"` and `\"sold_in_2_wks\"` are independent of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30819783-0f0d-41de-8fde-7c1e22b8366b",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "df[\"advertising\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6ef344-09cf-4f6b-bcc0-37c5a99ef76a",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "df[\"sold_in_2_wks\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7231488f-f623-4ee7-9dc8-53c5901b2718",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We then use the Chi-square test of independence and state the Null Hypothesis and Alternative Hypothesis:\n",
    "    \n",
    "- Null Hypothesis ($H_0$):  `\"advertising\"` and `\"sold_in_2_wks\"` are independent of each other.\n",
    "- Alternative Hypothesis ($H_a$): `\"advertising\"` and `\"sold_in_2_wks\"` dependent on each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd88094-d2d4-4cdd-89d0-54d5f758adb2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Contingency Tables\n",
    "\n",
    "In the Chi-Square test, we display the data in a cross-tabulation (contingency) format showing frequency count of each group of the two categorical variable rows and columns, which is call the **contingency table**. We can get the contingency table for `\"advertising\"` and `\"sold_in_2_wks\"` directly using Pandas `crosstab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6370a6-a3c7-451a-b563-aff295aa9ee8",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "tab = pd.crosstab(df[\"advertising\"], df[\"sold_in_2_wks\"])\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfabd45-c613-43cb-b732-b2b785fe549e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Alternatively, we can also use `statsmodels` to construct contingency table and conduct more analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ea4abe-51a8-42e5-b42f-d9674817dfc8",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import Table2x2\n",
    "\n",
    "contingency_table = Table2x2(tab.values)\n",
    "contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd19dd9-01eb-4b2f-82f7-4fba736bc07d",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Elements in contingency table\n",
    "contingency_table.table_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b81e4b9-5ed4-490b-8d50-aba30a75e314",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The fitted values are the estimated frequency counts for each cell under the assumption that `\"advertising\"` and `\"sold_in_2_wks\"` are independent to each other. Suppose the probability of `\"internet\"` and `\"radio\"` are $p$ and $1-p$, and the probability of `\"sold\"` and `\"unsold\"` is $q$ and $1-q$, the probability for each cell should be as follows:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th style=\"text-align: left\"></th>\n",
    "<th style=\"text-align: left\">$p$</th>\n",
    "<th style=\"text-align: left\">$1-p$</th>\n",
    "\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<td style=\"text-align: left\">$q$</td>\n",
    "<td style=\"text-align: left\">$p*q$</td>\n",
    "<td style=\"text-align: left\">$(1-p)*q$</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td style=\"text-align: left\">$1-q$</td>\n",
    "<td style=\"text-align: left\">$p*(1-q)$</td>\n",
    "<td style=\"text-align: left\">$(1-p)*(1-q)$</td>\n",
    "</tr>\n",
    "    \n",
    "</table>\n",
    "\n",
    "We can check the joint distribution of the contingency table through `independence_probabilities`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682571a7-714b-4730-af4c-bc974bea4aae",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "contingency_table.independence_probabilities.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4b8c6a-7b17-448d-bfae-d4f0285c7fcf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Then we apply the total sample size N to each cell's probability to calculate the fitted value. That's where the `fittedvalues` comes from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f145e86-a5e5-401e-b875-b5eca3be5fbe",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Get fitted value\n",
    "\n",
    "contingency_table.fittedvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e3f0c3-8682-421a-b483-e2e849703540",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Another useful information we can get from the probability of the contingency table is called the **odds ratio**. **odds** for an event with probability $p$ is: \n",
    "\n",
    "$$\\frac{p}{1-p}$$.\n",
    "\n",
    "The **odds ratio** is a ratio of **odds** between two groups, one with probability $p$ and one with probability $q$, the formula is:\n",
    "\n",
    "$$\\frac{p/(1-p)}{q/(1-q)}$$\n",
    "\n",
    "From the table of probability above, we can calculate odds ratio for the contingency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44927b2-8186-4998-9391-b7ed6ef3eaaa",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Get odds ratio\n",
    "\n",
    "contingency_table.oddsratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b081a64c-dd03-40e7-aeee-f06cf03d00f3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Test for Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97912582-bc0c-40fd-adb3-c82d42a6ffce",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Before moving to Chi square test statistic, we need to check whether we have enough observations to drive meaningful conclusions. For an effect size at `0.2`, significance level at 95% ($\\alpha$ is `0.05`), and power at `0.8`, we can use the `GofChisquarePower` function to conduct power analysis. First we check whether we have enough observations, then we can manipulate different effect sizes to see how sample sizes, power and effect sizes are correlated with each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beb0aae-8165-40f7-9813-8bdcb4597d62",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from statsmodels.stats.power import GofChisquarePower\n",
    "\n",
    "chi_square_power = GofChisquarePower()\n",
    "\n",
    "group_size = math.ceil(\n",
    "    chi_square_power.solve_power(effect_size=0.2, alpha=0.05, power=0.8)\n",
    ")\n",
    "\n",
    "print(\"Group size:\", group_size)\n",
    "print(\"Total # observations for two groups:\", group_size * 2)\n",
    "print(\"Total # observations in the data:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685e7ff8-a440-4b93-b7a7-0924874ee56b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In the following graph, let's see how effect size is correlated with sample size. If we assume power is at 0.8, to detect a 0.2 effect size, we need around 250 samples. While for an effect size at 0.5, we only need 50 observations, and the number reduced further for an even bigger effect size at 0.8. This means it is much harder to detect a smaller effect size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2490b272-0299-4958-aded-dfb80fa1d6c4",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_observations = np.arange(0, group_size * 2)\n",
    "effect_sizes = np.array([0.2, 0.5, 0.8])\n",
    "\n",
    "# Plot power curve using `chi_square_power`\n",
    "\n",
    "chi_square_power.plot_power(\n",
    "    dep_var=\"nobs\", nobs=n_observations, effect_size=effect_sizes, alpha=0.05, n_bins=2\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79197e53-8823-4640-974d-80b4739b3aeb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Lastly, we can conduct the test and calculate the p values using `test_nominal_association()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc6c99f-949f-4382-9f60-401b99179450",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "chi_square_test = contingency_table.test_nominal_association()\n",
    "\n",
    "print(\"chi_square_test type:\", type(chi_square_test))\n",
    "print(chi_square_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d696693-48c0-4126-898c-7f1aaa04fc4e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can see the *p-value* is around 0, which is much smaller than 0.05, the $\\alpha$ value we set ahead. We can then reject the Null Hypothesis, which states `\"advertising\"` and `\"sold_in_2_wks\"` are independent to each other. This is the same as saying `\"advertising\"` and `\"sold_in_2_wks\"` are somewhat dependent with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b65f50-2d58-4e38-a804-ef042384b7e4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# References & Further Reading \n",
    "\n",
    "- [Online Statistics Education: An Interactive Multimedia Course of Study](https://onlinestatbook.com/2/index.html)\n",
    "- [statsmodels documentation](https://www.statsmodels.org/stable/contingency_tables.html)\n",
    "- [Wikipedia: Normal Distribution](https://en.m.wikipedia.org/wiki/Normal_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6960349",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "Copyright © 2022 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
